{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cc10ce",
   "metadata": {},
   "source": [
    "# üìä Data Quality Analysis - Financial Transaction Data\n",
    "\n",
    "## üéØ Overview\n",
    "This notebook demonstrates the complete data quality analysis workflow for financial transaction data using our Data Quality Framework.\n",
    "\n",
    "**Objectives:**\n",
    "- Load and validate financial transaction data\n",
    "- Run comprehensive data quality checks\n",
    "- Analyze validation results and patterns\n",
    "- Generate detailed quality reports\n",
    "- Provide actionable insights for data governance\n",
    "\n",
    "**Author:** Srinivasan V  \n",
    "**Date:** July 2025  \n",
    "**Framework Version:** 1.0\n",
    "\n",
    "# Data Quality Framework for Financial Transactions\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a comprehensive data quality framework for financial transaction data using PySpark. It includes validation checks for mandatory fields, value ranges, duplicates, and referential integrity, along with automated reporting capabilities.\n",
    "\n",
    "## Key Features\n",
    "- ‚úÖ Modular, reusable data quality check functions\n",
    "- ‚úÖ Config-driven validation rules\n",
    "- ‚úÖ Failed records logging and inspection\n",
    "- ‚úÖ Weekly data quality reports\n",
    "- ‚úÖ PySpark for scalable data processing\n",
    "\n",
    "## Data Schema\n",
    "- **transaction_id**: Unique identifier for each transaction\n",
    "- **account_id**: Account identifier\n",
    "- **amount**: Transaction amount (must be > 0)\n",
    "- **currency**: Currency code (must be in approved list)\n",
    "- **timestamp**: Transaction timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86515565",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries and Set Up Spark Session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path to import our modules\n",
    "sys.path.append('../')\n",
    "from src.data_quality_framework import DataQualityFramework\n",
    "from src.validators import *\n",
    "from src.utils import load_config\n",
    "from src.report_generator import DataQualityReportGenerator\n",
    "\n",
    "# PySpark imports\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    PYSPARK_AVAILABLE = True\n",
    "    print(\"‚úì PySpark is available\")\n",
    "except ImportError:\n",
    "    PYSPARK_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è PySpark not available, falling back to Pandas\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Initialize Spark Session (if available)\n",
    "if PYSPARK_AVAILABLE:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataQualityFramework\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"‚úì Spark Session created successfully\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "else:\n",
    "    spark = None\n",
    "    print(\"Using Pandas for data processing\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199f41c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load Configuration Parameters\n",
    "config_base_path = \"../config\"\n",
    "\n",
    "# Load data quality configuration\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from JSON file\"\"\"\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è Config file not found: {config_path}\")\n",
    "        return {}\n",
    "\n",
    "# Load main configuration\n",
    "dq_config = load_config(os.path.join(config_base_path, \"data_quality_config.json\"))\n",
    "print(\"‚úì Data Quality Configuration loaded\")\n",
    "\n",
    "# Load approved currencies\n",
    "currencies_config = load_config(os.path.join(config_base_path, \"currencies.json\"))\n",
    "approved_currencies = currencies_config.get(\"approved_currencies\", [])\n",
    "print(f\"‚úì Approved currencies loaded: {len(approved_currencies)} currencies\")\n",
    "\n",
    "# Display configuration\n",
    "print(\"\\nüìã Configuration Summary:\")\n",
    "print(f\"Mandatory Fields: {dq_config.get('validation_rules', {}).get('mandatory_fields', [])}\")\n",
    "print(f\"Amount Range: {dq_config.get('validation_rules', {}).get('amount_validation', {})}\")\n",
    "print(f\"Sample Approved Currencies: {approved_currencies[:10]}...\")\n",
    "print(f\"Critical Pass Rate Threshold: {dq_config.get('data_quality_thresholds', {}).get('critical_pass_rate', 0.95)}\")\n",
    "print(f\"Warning Pass Rate Threshold: {dq_config.get('data_quality_thresholds', {}).get('warning_pass_rate', 0.90)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae9e37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load Financial Transactions Data\n",
    "data_path = \"../data/sample_transactions.csv\"\n",
    "\n",
    "# Load data using appropriate method\n",
    "if PYSPARK_AVAILABLE and spark:\n",
    "    # Load using PySpark\n",
    "    print(\"üìä Loading data with PySpark...\")\n",
    "    df_spark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(data_path)\n",
    "    \n",
    "    # Convert to Pandas for easier visualization and analysis\n",
    "    df = df_spark.toPandas()\n",
    "    \n",
    "    print(f\"‚úì Data loaded successfully using PySpark\")\n",
    "    print(f\"Dataset shape: {df_spark.count()} rows √ó {len(df_spark.columns)} columns\")\n",
    "else:\n",
    "    # Load using Pandas\n",
    "    print(\"üìä Loading data with Pandas...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    df_spark = None\n",
    "    \n",
    "    print(f\"‚úì Data loaded successfully using Pandas\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nüìà Dataset Overview:\")\n",
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã Sample Data:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display data types and missing values\n",
    "print(\"\\nüîç Data Types and Missing Values:\")\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Data Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Null Count': df.isnull().sum(),\n",
    "    'Null Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "display(info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b8d52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define Data Quality Check Functions\n",
    "\n",
    "def check_mandatory_fields(df, mandatory_fields):\n",
    "    \"\"\"\n",
    "    Check for non-null values in mandatory fields\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame (Pandas or PySpark)\n",
    "        mandatory_fields: List of mandatory field names\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with check results\n",
    "    \"\"\"\n",
    "    if PYSPARK_AVAILABLE and hasattr(df, 'sql_ctx'):\n",
    "        # PySpark implementation\n",
    "        total_records = df.count()\n",
    "        failed_records = df.filter(\n",
    "            reduce(lambda x, y: x | y, [col(field).isNull() for field in mandatory_fields])\n",
    "        )\n",
    "        failed_count = failed_records.count()\n",
    "    else:\n",
    "        # Pandas implementation\n",
    "        total_records = len(df)\n",
    "        mask = df[mandatory_fields].isnull().any(axis=1)\n",
    "        failed_count = mask.sum()\n",
    "        failed_records = df[mask]\n",
    "    \n",
    "    passed_count = total_records - failed_count\n",
    "    pass_rate = passed_count / total_records if total_records > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'check_name': 'mandatory_fields',\n",
    "        'total_records': total_records,\n",
    "        'passed_count': passed_count,\n",
    "        'failed_count': failed_count,\n",
    "        'pass_rate': pass_rate,\n",
    "        'failed_records': failed_records\n",
    "    }\n",
    "\n",
    "def check_amount_range(df, min_amount=0.01, max_amount=1000000.00):\n",
    "    \"\"\"\n",
    "    Check if amounts are within valid range\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        min_amount: Minimum valid amount\n",
    "        max_amount: Maximum valid amount\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with check results\n",
    "    \"\"\"\n",
    "    if PYSPARK_AVAILABLE and hasattr(df, 'sql_ctx'):\n",
    "        # PySpark implementation\n",
    "        total_records = df.count()\n",
    "        failed_records = df.filter(\n",
    "            (col(\"amount\") <= 0) | (col(\"amount\") > max_amount) | col(\"amount\").isNull()\n",
    "        )\n",
    "        failed_count = failed_records.count()\n",
    "    else:\n",
    "        # Pandas implementation\n",
    "        total_records = len(df)\n",
    "        mask = (df['amount'] <= 0) | (df['amount'] > max_amount) | df['amount'].isnull()\n",
    "        failed_count = mask.sum()\n",
    "        failed_records = df[mask]\n",
    "    \n",
    "    passed_count = total_records - failed_count\n",
    "    pass_rate = passed_count / total_records if total_records > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'check_name': 'amount_range',\n",
    "        'total_records': total_records,\n",
    "        'passed_count': passed_count,\n",
    "        'failed_count': failed_count,\n",
    "        'pass_rate': pass_rate,\n",
    "        'failed_records': failed_records,\n",
    "        'min_amount': min_amount,\n",
    "        'max_amount': max_amount\n",
    "    }\n",
    "\n",
    "def check_currency_codes(df, approved_currencies):\n",
    "    \"\"\"\n",
    "    Check if currency codes are in approved list\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        approved_currencies: List of approved currency codes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with check results\n",
    "    \"\"\"\n",
    "    if PYSPARK_AVAILABLE and hasattr(df, 'sql_ctx'):\n",
    "        # PySpark implementation\n",
    "        total_records = df.count()\n",
    "        failed_records = df.filter(\n",
    "            (~col(\"currency\").isin(approved_currencies)) | col(\"currency\").isNull()\n",
    "        )\n",
    "        failed_count = failed_records.count()\n",
    "    else:\n",
    "        # Pandas implementation\n",
    "        total_records = len(df)\n",
    "        mask = (~df['currency'].isin(approved_currencies)) | df['currency'].isnull()\n",
    "        failed_count = mask.sum()\n",
    "        failed_records = df[mask]\n",
    "    \n",
    "    passed_count = total_records - failed_count\n",
    "    pass_rate = passed_count / total_records if total_records > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'check_name': 'currency_codes',\n",
    "        'total_records': total_records,\n",
    "        'passed_count': passed_count,\n",
    "        'failed_count': failed_count,\n",
    "        'pass_rate': pass_rate,\n",
    "        'failed_records': failed_records,\n",
    "        'approved_currencies_count': len(approved_currencies)\n",
    "    }\n",
    "\n",
    "def check_duplicate_transactions(df, id_column='transaction_id'):\n",
    "    \"\"\"\n",
    "    Check for duplicate transaction IDs\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        id_column: Column name for transaction ID\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with check results\n",
    "    \"\"\"\n",
    "    if PYSPARK_AVAILABLE and hasattr(df, 'sql_ctx'):\n",
    "        # PySpark implementation\n",
    "        total_records = df.count()\n",
    "        # Find duplicates by counting occurrences\n",
    "        duplicate_ids = df.groupBy(id_column).count().filter(col(\"count\") > 1)\n",
    "        duplicate_transactions = df.join(duplicate_ids, id_column)\n",
    "        failed_count = duplicate_transactions.count()\n",
    "    else:\n",
    "        # Pandas implementation\n",
    "        total_records = len(df)\n",
    "        duplicates_mask = df.duplicated(subset=[id_column], keep=False)\n",
    "        failed_count = duplicates_mask.sum()\n",
    "        failed_records = df[duplicates_mask]\n",
    "    \n",
    "    passed_count = total_records - failed_count\n",
    "    pass_rate = passed_count / total_records if total_records > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'check_name': 'duplicate_transactions',\n",
    "        'total_records': total_records,\n",
    "        'passed_count': passed_count,\n",
    "        'failed_count': failed_count,\n",
    "        'pass_rate': pass_rate,\n",
    "        'failed_records': failed_records if not PYSPARK_AVAILABLE else duplicate_transactions\n",
    "    }\n",
    "\n",
    "def check_timestamp_format(df, timestamp_column='timestamp', date_format='yyyy-MM-dd HH:mm:ss'):\n",
    "    \"\"\"\n",
    "    Check timestamp format and validity\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        timestamp_column: Column name for timestamp\n",
    "        date_format: Expected date format\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with check results\n",
    "    \"\"\"\n",
    "    if PYSPARK_AVAILABLE and hasattr(df, 'sql_ctx'):\n",
    "        # PySpark implementation\n",
    "        total_records = df.count()\n",
    "        # Try to parse timestamp and check for nulls after parsing\n",
    "        df_with_parsed = df.withColumn(\n",
    "            \"parsed_timestamp\", \n",
    "            to_timestamp(col(timestamp_column), date_format)\n",
    "        )\n",
    "        failed_records = df_with_parsed.filter(\n",
    "            col(\"parsed_timestamp\").isNull() | col(timestamp_column).isNull()\n",
    "        )\n",
    "        failed_count = failed_records.count()\n",
    "    else:\n",
    "        # Pandas implementation\n",
    "        total_records = len(df)\n",
    "        try:\n",
    "            parsed_timestamps = pd.to_datetime(df[timestamp_column], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "            mask = parsed_timestamps.isnull() | df[timestamp_column].isnull()\n",
    "            failed_count = mask.sum()\n",
    "            failed_records = df[mask]\n",
    "        except:\n",
    "            failed_count = total_records\n",
    "            failed_records = df.copy()\n",
    "    \n",
    "    passed_count = total_records - failed_count\n",
    "    pass_rate = passed_count / total_records if total_records > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'check_name': 'timestamp_format',\n",
    "        'total_records': total_records,\n",
    "        'passed_count': passed_count,\n",
    "        'failed_count': failed_count,\n",
    "        'pass_rate': pass_rate,\n",
    "        'failed_records': failed_records,\n",
    "        'expected_format': date_format\n",
    "    }\n",
    "\n",
    "print(\"‚úì Data quality check functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a1812",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Data Quality Checks\n",
    "\n",
    "print(\"üîç Running Data Quality Checks...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Determine which DataFrame to use for checks\n",
    "working_df = df_spark if PYSPARK_AVAILABLE and df_spark else df\n",
    "\n",
    "# Initialize results storage\n",
    "validation_results = {}\n",
    "failed_records_collection = {}\n",
    "\n",
    "# 1. Check Mandatory Fields\n",
    "print(\"1Ô∏è‚É£ Checking mandatory fields...\")\n",
    "mandatory_fields = dq_config.get('validation_rules', {}).get('mandatory_fields', [])\n",
    "result_mandatory = check_mandatory_fields(working_df, mandatory_fields)\n",
    "validation_results['mandatory_fields'] = result_mandatory\n",
    "\n",
    "if result_mandatory['failed_count'] > 0:\n",
    "    failed_records_collection['mandatory_fields'] = result_mandatory['failed_records']\n",
    "\n",
    "print(f\"   ‚úì Pass Rate: {result_mandatory['pass_rate']:.2%}\")\n",
    "print(f\"   ‚úì Passed: {result_mandatory['passed_count']:,}\")\n",
    "print(f\"   ‚úó Failed: {result_mandatory['failed_count']:,}\")\n",
    "\n",
    "# 2. Check Amount Range\n",
    "print(\"\\n2Ô∏è‚É£ Checking amount range...\")\n",
    "amount_config = dq_config.get('validation_rules', {}).get('amount_validation', {})\n",
    "min_amount = amount_config.get('min_value', 0.01)\n",
    "max_amount = amount_config.get('max_value', 1000000.00)\n",
    "\n",
    "result_amount = check_amount_range(working_df, min_amount, max_amount)\n",
    "validation_results['amount_range'] = result_amount\n",
    "\n",
    "if result_amount['failed_count'] > 0:\n",
    "    failed_records_collection['amount_range'] = result_amount['failed_records']\n",
    "\n",
    "print(f\"   ‚úì Valid Range: ${min_amount:,.2f} - ${max_amount:,.2f}\")\n",
    "print(f\"   ‚úì Pass Rate: {result_amount['pass_rate']:.2%}\")\n",
    "print(f\"   ‚úì Passed: {result_amount['passed_count']:,}\")\n",
    "print(f\"   ‚úó Failed: {result_amount['failed_count']:,}\")\n",
    "\n",
    "# 3. Check Currency Codes\n",
    "print(\"\\n3Ô∏è‚É£ Checking currency codes...\")\n",
    "result_currency = check_currency_codes(working_df, approved_currencies)\n",
    "validation_results['currency_codes'] = result_currency\n",
    "\n",
    "if result_currency['failed_count'] > 0:\n",
    "    failed_records_collection['currency_codes'] = result_currency['failed_records']\n",
    "\n",
    "print(f\"   ‚úì Approved Currencies: {len(approved_currencies)}\")\n",
    "print(f\"   ‚úì Pass Rate: {result_currency['pass_rate']:.2%}\")\n",
    "print(f\"   ‚úì Passed: {result_currency['passed_count']:,}\")\n",
    "print(f\"   ‚úó Failed: {result_currency['failed_count']:,}\")\n",
    "\n",
    "# 4. Check Duplicate Transactions\n",
    "print(\"\\n4Ô∏è‚É£ Checking for duplicate transactions...\")\n",
    "result_duplicates = check_duplicate_transactions(working_df)\n",
    "validation_results['duplicate_transactions'] = result_duplicates\n",
    "\n",
    "if result_duplicates['failed_count'] > 0:\n",
    "    failed_records_collection['duplicate_transactions'] = result_duplicates['failed_records']\n",
    "\n",
    "print(f\"   ‚úì Pass Rate: {result_duplicates['pass_rate']:.2%}\")\n",
    "print(f\"   ‚úì Passed: {result_duplicates['passed_count']:,}\")\n",
    "print(f\"   ‚úó Failed: {result_duplicates['failed_count']:,}\")\n",
    "\n",
    "# 5. Check Timestamp Format\n",
    "print(\"\\n5Ô∏è‚É£ Checking timestamp format...\")\n",
    "result_timestamp = check_timestamp_format(working_df)\n",
    "validation_results['timestamp_format'] = result_timestamp\n",
    "\n",
    "if result_timestamp['failed_count'] > 0:\n",
    "    failed_records_collection['timestamp_format'] = result_timestamp['failed_records']\n",
    "\n",
    "print(f\"   ‚úì Expected Format: YYYY-MM-DD HH:MM:SS\")\n",
    "print(f\"   ‚úì Pass Rate: {result_timestamp['pass_rate']:.2%}\")\n",
    "print(f\"   ‚úì Passed: {result_timestamp['passed_count']:,}\")\n",
    "print(f\"   ‚úó Failed: {result_timestamp['failed_count']:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ Data Quality Checks Completed!\")\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_records = len(df)\n",
    "total_failed = sum([result['failed_count'] for result in validation_results.values()])\n",
    "overall_pass_rate = (total_records - total_failed) / total_records if total_records > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Overall Summary:\")\n",
    "print(f\"   üìà Total Records: {total_records:,}\")\n",
    "print(f\"   ‚úÖ Total Passed: {total_records - total_failed:,}\")\n",
    "print(f\"   ‚ùå Total Failed: {total_failed:,}\")\n",
    "print(f\"   üìà Overall Pass Rate: {overall_pass_rate:.2%}\")\n",
    "\n",
    "# Determine quality status\n",
    "critical_threshold = dq_config.get('data_quality_thresholds', {}).get('critical_pass_rate', 0.95)\n",
    "warning_threshold = dq_config.get('data_quality_thresholds', {}).get('warning_pass_rate', 0.90)\n",
    "\n",
    "if overall_pass_rate >= critical_threshold:\n",
    "    status = \"üü¢ EXCELLENT\"\n",
    "elif overall_pass_rate >= warning_threshold:\n",
    "    status = \"üü° WARNING\"\n",
    "else:\n",
    "    status = \"üî¥ CRITICAL\"\n",
    "\n",
    "print(f\"   üèÜ Quality Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483eb7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Store Failed Records for Inspection\n",
    "\n",
    "print(\"üíæ Storing Failed Records for Inspection...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create directory for failed records if it doesn't exist\n",
    "failed_records_dir = \"../data/failed_records\"\n",
    "os.makedirs(failed_records_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "saved_files = []\n",
    "\n",
    "# Store each type of failed records\n",
    "for check_name, failed_df in failed_records_collection.items():\n",
    "    if PYSPARK_AVAILABLE and hasattr(failed_df, 'sql_ctx'):\n",
    "        # Convert PySpark DataFrame to Pandas for saving\n",
    "        failed_pandas = failed_df.toPandas()\n",
    "    else:\n",
    "        failed_pandas = failed_df.copy()\n",
    "    \n",
    "    if len(failed_pandas) > 0:\n",
    "        # Add metadata columns\n",
    "        failed_pandas['validation_check'] = check_name\n",
    "        failed_pandas['validation_timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        failed_pandas['failure_reason'] = f\"Failed {check_name.replace('_', ' ')} validation\"\n",
    "        \n",
    "        # Save to CSV\n",
    "        filename = f\"failed_{check_name}_{timestamp}.csv\"\n",
    "        filepath = os.path.join(failed_records_dir, filename)\n",
    "        failed_pandas.to_csv(filepath, index=False)\n",
    "        \n",
    "        saved_files.append(filepath)\n",
    "        print(f\"‚úì Saved {len(failed_pandas)} failed records: {filename}\")\n",
    "\n",
    "# Create a comprehensive failed records summary\n",
    "if saved_files:\n",
    "    print(f\"\\nüìÅ Failed records saved to: {failed_records_dir}\")\n",
    "    print(f\"üìÑ Files created: {len(saved_files)}\")\n",
    "    \n",
    "    # Create a summary of all failed records\n",
    "    all_failed_records = []\n",
    "    for check_name, failed_df in failed_records_collection.items():\n",
    "        if PYSPARK_AVAILABLE and hasattr(failed_df, 'sql_ctx'):\n",
    "            failed_pandas = failed_df.toPandas()\n",
    "        else:\n",
    "            failed_pandas = failed_df.copy()\n",
    "        \n",
    "        if len(failed_pandas) > 0:\n",
    "            failed_pandas['validation_check'] = check_name\n",
    "            all_failed_records.append(failed_pandas)\n",
    "    \n",
    "    if all_failed_records:\n",
    "        combined_failed = pd.concat(all_failed_records, ignore_index=True)\n",
    "        combined_filename = f\"all_failed_records_{timestamp}.csv\"\n",
    "        combined_filepath = os.path.join(failed_records_dir, combined_filename)\n",
    "        combined_failed.to_csv(combined_filepath, index=False)\n",
    "        \n",
    "        print(f\"‚úì Combined failed records saved: {combined_filename}\")\n",
    "        print(f\"üìä Total failed records across all checks: {len(combined_failed)}\")\n",
    "        \n",
    "        # Display sample of failed records\n",
    "        print(f\"\\nüîç Sample Failed Records:\")\n",
    "        display(combined_failed.head(10))\n",
    "        \n",
    "        # Failed records by validation check\n",
    "        failed_by_check = combined_failed['validation_check'].value_counts()\n",
    "        print(f\"\\nüìä Failed Records by Validation Check:\")\n",
    "        for check, count in failed_by_check.items():\n",
    "            print(f\"   {check.replace('_', ' ').title()}: {count:,}\")\n",
    "            \n",
    "else:\n",
    "    print(\"üéâ No failed records found! All data passed validation checks.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecef500",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate Data Quality Results and Visualizations\n",
    "\n",
    "print(\"üìä Creating Data Quality Visualizations...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create summary DataFrame for easier analysis\n",
    "summary_data = []\n",
    "for check_name, result in validation_results.items():\n",
    "    summary_data.append({\n",
    "        'Validation_Check': check_name.replace('_', ' ').title(),\n",
    "        'Total_Records': result['total_records'],\n",
    "        'Passed_Count': result['passed_count'],\n",
    "        'Failed_Count': result['failed_count'],\n",
    "        'Pass_Rate': result['pass_rate'],\n",
    "        'Pass_Rate_Percent': f\"{result['pass_rate']:.2%}\",\n",
    "        'Status': '‚úÖ PASS' if result['pass_rate'] >= 0.95 else '‚ö†Ô∏è WARNING' if result['pass_rate'] >= 0.90 else '‚ùå FAIL'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"üìã Data Quality Summary Table:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Create visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Pass Rates by Validation Check\n",
    "checks = summary_df['Validation_Check']\n",
    "pass_rates = summary_df['Pass_Rate'] * 100\n",
    "\n",
    "colors = ['green' if rate >= 95 else 'orange' if rate >= 90 else 'red' for rate in pass_rates]\n",
    "bars1 = ax1.bar(checks, pass_rates, color=colors, alpha=0.7)\n",
    "ax1.set_title('Pass Rates by Validation Check', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Pass Rate (%)')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rate in zip(bars1, pass_rates):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Failed Records Count\n",
    "failed_counts = summary_df['Failed_Count']\n",
    "bars2 = ax2.bar(checks, failed_counts, color='red', alpha=0.7)\n",
    "ax2.set_title('Failed Records by Validation Check', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Failed Records')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars2, failed_counts):\n",
    "    if count > 0:\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Overall Quality Distribution Pie Chart\n",
    "total_passed = total_records - total_failed\n",
    "sizes = [total_passed, total_failed]\n",
    "labels = ['Passed Records', 'Failed Records']\n",
    "colors_pie = ['green', 'red']\n",
    "\n",
    "if total_failed > 0:\n",
    "    wedges, texts, autotexts = ax3.pie(sizes, labels=labels, autopct='%1.1f%%', \n",
    "                                      colors=colors_pie, startangle=90)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "else:\n",
    "    ax3.pie([100], labels=['All Records Passed'], colors=['green'], autopct='%1.1f%%')\n",
    "\n",
    "ax3.set_title('Overall Data Quality Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Quality Score Gauge\n",
    "overall_score = overall_pass_rate * 100\n",
    "ax4.barh(['Overall Quality Score'], [overall_score], \n",
    "         color='green' if overall_score >= 95 else 'orange' if overall_score >= 90 else 'red',\n",
    "         alpha=0.7)\n",
    "ax4.set_xlim(0, 100)\n",
    "ax4.set_xlabel('Quality Score (%)')\n",
    "ax4.set_title(f'Overall Quality Score: {overall_score:.1f}%', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add score text\n",
    "ax4.text(overall_score/2, 0, f'{overall_score:.1f}%', \n",
    "         ha='center', va='center', fontweight='bold', fontsize=12, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis: Currency distribution\n",
    "print(\"\\nüí± Currency Distribution Analysis:\")\n",
    "if 'currency' in df.columns:\n",
    "    currency_counts = df['currency'].value_counts()\n",
    "    print(\"Top 10 currencies by transaction count:\")\n",
    "    display(currency_counts.head(10))\n",
    "    \n",
    "    # Check which currencies are not in approved list\n",
    "    invalid_currencies = df[~df['currency'].isin(approved_currencies)]['currency'].value_counts()\n",
    "    if len(invalid_currencies) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Invalid currencies found ({len(invalid_currencies)} unique):\")\n",
    "        display(invalid_currencies.head(10))\n",
    "\n",
    "# Amount distribution analysis\n",
    "print(\"\\nüí∞ Amount Distribution Analysis:\")\n",
    "if 'amount' in df.columns:\n",
    "    amount_stats = df['amount'].describe()\n",
    "    print(\"Amount statistics:\")\n",
    "    display(amount_stats)\n",
    "    \n",
    "    # Plot amount distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    df['amount'].hist(bins=50, ax=ax1, alpha=0.7, color='blue')\n",
    "    ax1.set_title('Amount Distribution')\n",
    "    ax1.set_xlabel('Amount')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.axvline(df['amount'].mean(), color='red', linestyle='--', label=f'Mean: ${df[\"amount\"].mean():.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Box plot\n",
    "    df.boxplot(column='amount', ax=ax2)\n",
    "    ax2.set_title('Amount Box Plot')\n",
    "    ax2.set_ylabel('Amount')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29acb18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Weekly Data Quality Report\n",
    "\n",
    "print(\"üìã Generating Weekly Data Quality Report...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create report data structure\n",
    "week_start = datetime.now() - timedelta(days=datetime.now().weekday())\n",
    "week_end = week_start + timedelta(days=6)\n",
    "report_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Executive Summary\n",
    "executive_summary = {\n",
    "    'Report_Period': f\"{week_start.strftime('%Y-%m-%d')} to {week_end.strftime('%Y-%m-%d')}\",\n",
    "    'Report_Generated': report_timestamp,\n",
    "    'Total_Records_Processed': total_records,\n",
    "    'Total_Passed_Records': total_records - total_failed,\n",
    "    'Total_Failed_Records': total_failed,\n",
    "    'Overall_Pass_Rate': f\"{overall_pass_rate:.2%}\",\n",
    "    'Quality_Status': status.split()[-1],  # Extract just the status word\n",
    "    'Data_Quality_Score': f\"{overall_pass_rate * 100:.1f}/100\"\n",
    "}\n",
    "\n",
    "print(\"üìä Executive Summary:\")\n",
    "for key, value in executive_summary.items():\n",
    "    print(f\"   {key.replace('_', ' ')}: {value}\")\n",
    "\n",
    "# Detailed Results Table\n",
    "print(f\"\\nüìã Detailed Validation Results:\")\n",
    "detailed_results = summary_df[['Validation_Check', 'Total_Records', 'Passed_Count', \n",
    "                              'Failed_Count', 'Pass_Rate_Percent', 'Status']].copy()\n",
    "display(detailed_results)\n",
    "\n",
    "# Failed Records Summary\n",
    "if failed_records_collection:\n",
    "    print(f\"\\n‚ùå Failed Records Summary:\")\n",
    "    failed_summary = []\n",
    "    for check_name, failed_df in failed_records_collection.items():\n",
    "        if PYSPARK_AVAILABLE and hasattr(failed_df, 'sql_ctx'):\n",
    "            count = failed_df.count()\n",
    "        else:\n",
    "            count = len(failed_df)\n",
    "        \n",
    "        if count > 0:\n",
    "            failed_summary.append({\n",
    "                'Validation_Check': check_name.replace('_', ' ').title(),\n",
    "                'Failed_Count': count,\n",
    "                'Percentage_of_Total': f\"{(count / total_records * 100):.2f}%\"\n",
    "            })\n",
    "    \n",
    "    if failed_summary:\n",
    "        failed_summary_df = pd.DataFrame(failed_summary)\n",
    "        display(failed_summary_df)\n",
    "\n",
    "# Recommendations based on results\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "recommendations = []\n",
    "\n",
    "for check_name, result in validation_results.items():\n",
    "    pass_rate = result['pass_rate']\n",
    "    check_display_name = check_name.replace('_', ' ').title()\n",
    "    \n",
    "    if pass_rate < 0.90:\n",
    "        if check_name == 'mandatory_fields':\n",
    "            recommendations.append(f\"üî¥ {check_display_name}: Immediate action required. Review data ingestion process for mandatory field validation.\")\n",
    "        elif check_name == 'amount_range':\n",
    "            recommendations.append(f\"üî¥ {check_display_name}: Review transaction amount validation rules and data sources.\")\n",
    "        elif check_name == 'currency_codes':\n",
    "            recommendations.append(f\"üî¥ {check_display_name}: Update currency validation list or review data sources for invalid currencies.\")\n",
    "        elif check_name == 'duplicate_transactions':\n",
    "            recommendations.append(f\"üî¥ {check_display_name}: Investigate transaction ID generation process to prevent duplicates.\")\n",
    "        elif check_name == 'timestamp_format':\n",
    "            recommendations.append(f\"üî¥ {check_display_name}: Standardize timestamp format across all data sources.\")\n",
    "    elif pass_rate < 0.95:\n",
    "        recommendations.append(f\"üü° {check_display_name}: Monitor closely. Pass rate below excellent threshold.\")\n",
    "    else:\n",
    "        recommendations.append(f\"üü¢ {check_display_name}: Excellent quality. Maintain current standards.\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "# Export report to files\n",
    "reports_dir = \"../data/reports\"\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "# Excel Report\n",
    "report_timestamp_file = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "excel_filename = f\"Weekly_DQ_Report_{week_start.strftime('%Y%m%d')}_{report_timestamp_file}.xlsx\"\n",
    "excel_filepath = os.path.join(reports_dir, excel_filename)\n",
    "\n",
    "try:\n",
    "    with pd.ExcelWriter(excel_filepath, engine='openpyxl') as writer:\n",
    "        # Executive Summary Sheet\n",
    "        summary_sheet_data = pd.DataFrame(list(executive_summary.items()), \n",
    "                                        columns=['Metric', 'Value'])\n",
    "        summary_sheet_data.to_excel(writer, sheet_name='Executive_Summary', index=False)\n",
    "        \n",
    "        # Detailed Results Sheet\n",
    "        detailed_results.to_excel(writer, sheet_name='Validation_Details', index=False)\n",
    "        \n",
    "        # Failed Records Summary\n",
    "        if failed_summary:\n",
    "            failed_summary_df.to_excel(writer, sheet_name='Failed_Records_Summary', index=False)\n",
    "        \n",
    "        # Raw Data Sample\n",
    "        df.head(100).to_excel(writer, sheet_name='Data_Sample', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Excel report saved: {excel_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not create Excel report: {e}\")\n",
    "\n",
    "# HTML Report\n",
    "html_filename = f\"Weekly_DQ_Report_{week_start.strftime('%Y%m%d')}.html\"\n",
    "html_filepath = os.path.join(reports_dir, html_filename)\n",
    "\n",
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Weekly Data Quality Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}\n",
    "        .header {{ background-color: #f0f8ff; padding: 20px; border-radius: 10px; margin-bottom: 20px; }}\n",
    "        .summary {{ background-color: #f9f9f9; padding: 15px; border-radius: 5px; margin: 20px 0; }}\n",
    "        .excellent {{ color: #28a745; }}\n",
    "        .warning {{ color: #ffc107; }}\n",
    "        .critical {{ color: #dc3545; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
    "        th {{ background-color: #f2f2f2; font-weight: bold; }}\n",
    "        .metric {{ font-size: 1.2em; margin: 10px 0; }}\n",
    "        .status-excellent {{ background-color: #d4edda; color: #155724; }}\n",
    "        .status-warning {{ background-color: #fff3cd; color: #856404; }}\n",
    "        .status-critical {{ background-color: #f8d7da; color: #721c24; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>üìä Weekly Data Quality Report</h1>\n",
    "        <p><strong>Report Period:</strong> {executive_summary['Report_Period']}</p>\n",
    "        <p><strong>Generated:</strong> {executive_summary['Report_Generated']}</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"summary\">\n",
    "        <h2>üéØ Executive Summary</h2>\n",
    "        <div class=\"metric\">üìà Total Records: <strong>{executive_summary['Total_Records_Processed']:,}</strong></div>\n",
    "        <div class=\"metric\">‚úÖ Passed Records: <strong>{executive_summary['Total_Passed_Records']:,}</strong></div>\n",
    "        <div class=\"metric\">‚ùå Failed Records: <strong>{executive_summary['Total_Failed_Records']:,}</strong></div>\n",
    "        <div class=\"metric\">üìä Pass Rate: <strong>{executive_summary['Overall_Pass_Rate']}</strong></div>\n",
    "        <div class=\"metric\">üèÜ Quality Status: <strong class=\"{executive_summary['Quality_Status'].lower()}\">{executive_summary['Quality_Status']}</strong></div>\n",
    "        <div class=\"metric\">üìà Quality Score: <strong>{executive_summary['Data_Quality_Score']}</strong></div>\n",
    "    </div>\n",
    "    \n",
    "    <h2>üìã Detailed Validation Results</h2>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Validation Check</th>\n",
    "            <th>Total Records</th>\n",
    "            <th>Passed</th>\n",
    "            <th>Failed</th>\n",
    "            <th>Pass Rate</th>\n",
    "            <th>Status</th>\n",
    "        </tr>\n",
    "\"\"\"\n",
    "\n",
    "for _, row in detailed_results.iterrows():\n",
    "    status_class = 'excellent' if '‚úÖ' in row['Status'] else 'warning' if '‚ö†Ô∏è' in row['Status'] else 'critical'\n",
    "    html_content += f\"\"\"\n",
    "        <tr>\n",
    "            <td>{row['Validation_Check']}</td>\n",
    "            <td>{row['Total_Records']:,}</td>\n",
    "            <td>{row['Passed_Count']:,}</td>\n",
    "            <td>{row['Failed_Count']:,}</td>\n",
    "            <td>{row['Pass_Rate_Percent']}</td>\n",
    "            <td class=\"status-{status_class}\">{row['Status']}</td>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "\n",
    "html_content += f\"\"\"\n",
    "    </table>\n",
    "    \n",
    "    <h2>üí° Key Recommendations</h2>\n",
    "    <ul>\n",
    "\"\"\"\n",
    "\n",
    "for rec in recommendations:\n",
    "    html_content += f\"<li>{rec}</li>\"\n",
    "\n",
    "html_content += \"\"\"\n",
    "    </ul>\n",
    "    \n",
    "    <div class=\"summary\" style=\"margin-top: 40px;\">\n",
    "        <p><strong>Note:</strong> This report was automatically generated by the Data Quality Framework.</p>\n",
    "        <p>For detailed failed records analysis, please refer to the failed records CSV files in the data/failed_records directory.</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(html_filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"‚úÖ HTML report saved: {html_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not create HTML report: {e}\")\n",
    "\n",
    "print(f\"\\nüìÅ Reports saved to: {reports_dir}\")\n",
    "print(f\"üìÑ Available formats: Excel, HTML\")\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   1. Review failed records in: ../data/failed_records/\")\n",
    "print(f\"   2. Open reports in: ../data/reports/\")\n",
    "print(f\"   3. Implement recommended actions\")\n",
    "print(f\"   4. Schedule weekly report generation\")\n",
    "\n",
    "print(\"\\n‚úÖ Weekly Data Quality Report Generation Completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
